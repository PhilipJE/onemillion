{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef83b60-3e7a-4789-8455-057c2a8a8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import json\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dca880e-2810-4ca6-8ce7-b834af238d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/16 23:59:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cores = 2\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.156:7077\") \\\n",
    "        .appName(\"Group 6 project 2 cores\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", \"2\")\\\n",
    "        .config(\"spark.cores.max\", 2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://192.168.2.156:9000\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark_session.sparkContext)\n",
    "spark_session.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea49151-abf3-41fa-a6d1-4972ba654240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:====>                                                   (12 + 2) / 147]"
     ]
    }
   ],
   "source": [
    "# Load JSON as a Spark DataFrame\n",
    "df = spark_session.read.json(\"hdfs://192.168.2.156:9000/data/reddit/corpus-webis-tldr-17.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47315dc9-b4bf-4b11-8cb2-0fcde52c1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show() #Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f217d52-99ac-4f56-ab89-7352a5fec993",
   "metadata": {},
   "source": [
    "# Task 1: Reading level in subreddits\n",
    "First, we analyse the language level using Fleschâ€“Kincaid grade level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879328a-533c-49c3-8df8-ba297548e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting timer\n",
    "t_start = time()\n",
    "\n",
    "# Making a new df for analyzing the reading level\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "# Splitting sentences\n",
    "df_spell = df.select(\"subreddit\", split(df[\"content\"], r'[.!?]').alias(\"content_split\"))\n",
    "\n",
    "# Splitting words, removing punctuation and empty strings\n",
    "df_words = df_spell.withColumn(\"content_split\", expr(\"\"\"transform(content_split, x -> filter(transform(split(x, ' '), word -> trim(regexp_replace(regexp_replace(word, '[\\\\n]', ''), '[,\\\\.\\\\!\\\\?:\\\\*\\\\(\\\\)]', ''))), word -> word != ''))\"\"\"))\n",
    "\n",
    "df_words = df_words.filter(size(df_words[\"content_split\"]) > 3)  # Remove empty rows\n",
    "\n",
    "df_words.first()\n",
    "print(f\"The length is {df_words.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4da8b9-0e71-42f1-b50f-8a64d14ce981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the syllable count function\n",
    "def syllable_count(word):\n",
    "    # If the word is not alphabetic, return 0 (e.g., for numbers or symbols)\n",
    "    if not word.isalpha():\n",
    "        return 0\n",
    "    \n",
    "    word = word.lower()  # Ensure the word is in lowercase\n",
    "    count = 0\n",
    "    vowels = \"aeiou\"\n",
    "    \n",
    "    # Check the first letter\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    \n",
    "    # Count syllables in the rest of the word\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    \n",
    "    # Special case: Subtract 1 if the word ends with 'e' (unless 'e' is preceded by a vowel)\n",
    "    if word.endswith(\"e\") and (len(word) > 1 and word[-2] not in vowels):\n",
    "        count -= 1\n",
    "    \n",
    "    # Ensure that a word has at least one syllable\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    \n",
    "    # Limit reasonable syllable count (this can be adjusted based on context)\n",
    "    if count < 10:  # Reasonable cap for most English words\n",
    "        return count\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Define the function to calculate the reading level (Flesch-Kincaid)\n",
    "def reading_level(list_of_sentences):\n",
    "    # Flatten the list of lists into a single list of words\n",
    "\n",
    "    total_words = 0\n",
    "    total_syllables = 0\n",
    "    total_sentences = 0  # Since each row represents one sentence (as split by punctuation)\n",
    "\n",
    "    # Iterate over each word in the flattened list and calculate total words and syllables\n",
    "    for sentence in list_of_sentences:\n",
    "        \n",
    "        words_in_sentence = 0\n",
    "        syllables_in_sentence = 0\n",
    "        \n",
    "        # Iterate over each word in the sentence\n",
    "        for word in sentence:\n",
    "            # Skip empty strings or unwanted characters (e.g., newlines)\n",
    "            word = word.strip(\",. !?:*()[]\")\n",
    "            if word.isalpha():  # Ensure the word is alphabetic\n",
    "                words_in_sentence += 1  # Increment word count\n",
    "                syllables_in_sentence += syllable_count(word)  # Add syllables for the word\n",
    "                \n",
    "        if words_in_sentence > 0 and words_in_sentence < 35: # Only add reasonable values\n",
    "            total_words += words_in_sentence\n",
    "            total_syllables += syllables_in_sentence\n",
    "            total_sentences += 1\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if total_words == 0 or total_sentences == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate the Flesch-Kincaid readability index with the formula:\n",
    "    # Reading Level = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n",
    "    level = 0.39*(total_words / total_sentences) + 11.8*(total_syllables / total_words) - 15.59\n",
    "    if level < -3.5:\n",
    "        raise ValueError(f\"Level too low ({level}). Total words: {total_words}, Total sentences: {total_sentences}, Total syllables: {total_syllables}\")\n",
    "    else:\n",
    "        return level\n",
    "\n",
    "# # Define the UDF for reading level calculation\n",
    "# def reading_level_udf(sentence_list):\n",
    "#     return float(reading_level(sentence_list))\n",
    "\n",
    "# Register the UDF\n",
    "udf_reading_level = udf(lambda sentence_list: reading_level(sentence_list))\n",
    "\n",
    "# Apply the UDF to calculate the reading level for each row\n",
    "df_with_reading_level = df_words.withColumn(\"reading_level\", udf_reading_level(\"content_split\"))\n",
    "\n",
    "# Show the result (debug)\n",
    "df_with_reading_level.select(\"subreddit\", \"reading_level\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0aad15-34da-4632-8a9f-1d7b99e7cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average_grade = df_with_reading_level.groupBy(\"subreddit\").agg(avg(\"reading_level\").alias(\"Avg_reading_level\"))\n",
    "# df_average_grade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06336e15-a8dc-4c18-9955-90a9b90f83d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top 10 subreddits with the worst (lowest) reading level\n",
    "top_10_best = df_average_grade.orderBy(\"avg_reading_level\", ascending=True).limit(10)\n",
    "\n",
    "# Top 10 subreddits with the best (highest) reading level\n",
    "top_10_worst = df_average_grade.orderBy(\"avg_reading_level\", ascending=False).limit(10)\n",
    "\n",
    "print(\"Top 10 best Subreddits:\")\n",
    "top_10_worst.select(\"subreddit\", \"avg_reading_level\").show(truncate=False)\n",
    "\n",
    "print(\"Top 10 worst Subreddits:\")\n",
    "top_10_best.select(\"subreddit\", \"avg_reading_level\").show(truncate=False)\n",
    "\n",
    "t_end = time()\n",
    "time_taken = t_end - t_start\n",
    "\n",
    "print(f\"Total time taken: {time_taken}\\nCores used: {cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212e91e-5a1f-456f-9d7b-c677dc7f814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df_with_reading_level.select(\"reading_level\").limit(500).toPandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot histogram\n",
    "plt.hist(pdf[\"reading_level\"], edgecolor=\"black\")\n",
    "plt.xlabel(\"Reading Level\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Reading Levels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1dbf2f-6ff0-4a02-a15a-516ddd0f3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot histogram\n",
    "plt.hist(pdf[\"reading_level\"], bins=30, edgecolor=\"black\")\n",
    "plt.xlabel(\"Reading Level\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Reading Levels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa3e07-9e58-4f02-8096-db7eeebe710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "webkins = df.where(\"subreddit\" == \"Webkinz\")\n",
    "webkins.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70270ab-fd7a-492b-9bd9-6ae36558a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791cbf03-a7d1-49e6-a79a-d9c1bf72a4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
